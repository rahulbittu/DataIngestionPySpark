# Data Sources Configuration for the Data Ingestion Pipeline

# Default classification thresholds for determining data quality
default_classification:
  bronze:
    completeness: 0.7
    accuracy: 0.6
    timeliness: 0.5
  silver:
    completeness: 0.8
    accuracy: 0.8
    timeliness: 0.7
  gold:
    completeness: 0.95
    accuracy: 0.9
    timeliness: 0.85

# Target paths for storing classified data
target_paths:
  bronze: "/data/bronze"
  silver: "/data/silver"
  gold: "/data/gold"
  rejected: "/data/rejected"

# File-based data sources
file_sources:
  - name: "customer_data"
    type: "csv"
    path: "/data/sources/customers.csv"
    schema:
      - name: "customer_id"
        type: "string"
        required: true
      - name: "name"
        type: "string"
        required: true
      - name: "email"
        type: "string"
        required: true
      - name: "phone"
        type: "string"
        required: false
      - name: "address"
        type: "string"
        required: false
      - name: "created_at"
        type: "timestamp"
        required: true
    options:
      header: true
      inferSchema: false

  - name: "product_catalog"
    type: "parquet"
    path: "/data/sources/products.parquet"
    options:
      mergeSchema: true

# Database data sources
database_sources:
  - name: "sales_transactions"
    connection:
      type: "postgresql"
      host: "${DB_HOST}"
      port: "${DB_PORT}"
      database: "${DB_NAME}"
      user: "${DB_USER}"
      password: "${DB_PASSWORD}"
    table: "sales"
    query: "SELECT * FROM sales WHERE transaction_date >= current_date - interval '7 days'"
    options:
      fetchsize: 10000

  - name: "inventory_levels"
    connection:
      type: "mysql"
      host: "${MYSQL_HOST}"
      port: "${MYSQL_PORT}"
      database: "${MYSQL_DB}"
      user: "${MYSQL_USER}"
      password: "${MYSQL_PASSWORD}"
    table: "inventory"
    options:
      partitionColumn: "product_id"
      lowerBound: 1
      upperBound: 100000
      numPartitions: 10

# API data sources
api_sources:
  - name: "weather_data"
    base_url: "https://api.weather.example.com/v1"
    endpoint: "/current"
    method: "GET"
    headers:
      Content-Type: "application/json"
      Authorization: "Bearer ${WEATHER_API_KEY}"
    params:
      locations: "London,New York,Tokyo,Sydney"
      units: "metric"
    response_format: "json"
    response_path: "data"
    pagination:
      enabled: true
      limit_param: "limit"
      offset_param: "offset"
      limit: 100
      max_pages: 10

  - name: "exchange_rates"
    base_url: "https://api.exchangerate.example.com"
    endpoint: "/latest"
    method: "GET"
    headers:
      apikey: "${EXCHANGE_API_KEY}"
    params:
      base: "USD"
      symbols: "EUR,GBP,JPY,CAD,AUD"
    response_format: "json"
    response_path: "rates"
    caching:
      enabled: true
      ttl_seconds: 3600

# NVD data sources (CVE information)
nvd_sources:
  - name: "nvd_cve_data"
    type: "nvd"
    api_key: "${NVD_API_KEY}"  # Optional: NVD API key for higher rate limits
    delay_between_requests: 6  # Default delay in seconds to respect rate limits
    results_per_page: 2000     # Number of results per page (max 2000)
    max_pages: 5               # Optional: Limit number of pages to fetch
    start_date: "2023-01-01T00:00:00.000"  # Optional: Start date for CVE filtering
    end_date: "2023-12-31T23:59:59.999"    # Optional: End date for CVE filtering
    additional_query_params:    # Optional: Additional query parameters
      cvssV3Severity: "HIGH"    # Filter by CVSS V3 severity
      # Other potential filters:
      # keywordSearch: "log4j"  # Search for specific keywords
      # isVulnerable: true      # Filter for confirmed vulnerabilities
      # cveId: "CVE-2021-44228" # Filter for specific CVE ID
    incremental_load: true     # Whether to use incremental loading
    state_path: "./nvd_state"  # Directory to store state information for incremental loads
    retention_period_days: 90  # How long to keep the data
    enabled: true              # Whether this source is enabled

# Kafka data sources
kafka_sources:
  - name: "user_events"
    bootstrap_servers: "${KAFKA_BOOTSTRAP_SERVERS}"
    topic: "user-events"
    group_id: "data-ingestion-pipeline"
    schema_registry_url: "${SCHEMA_REGISTRY_URL}"
    starting_offsets: "latest"
    options:
      failOnDataLoss: false
      maxOffsetsPerTrigger: 10000
    value_schema: "avro"
    authentication:
      type: "SASL_SSL"
      username: "${KAFKA_USERNAME}"
      password: "${KAFKA_PASSWORD}"

  - name: "system_metrics"
    bootstrap_servers: "${KAFKA_BOOTSTRAP_SERVERS}"
    topic: "system-metrics"
    group_id: "data-ingestion-pipeline"
    starting_offsets: "earliest"
    options:
      includeHeaders: true

# Hive configuration (primary data store)
hive:
  name: "hive_default"
  type: "hive"
  database: "${HIVE_DATABASE:default}"
  host: "${HIVE_HOST:localhost}"
  port: ${HIVE_PORT:10000}
  auth_mechanism: "${HIVE_AUTH:NONE}"
  username: "${HIVE_USERNAME}"
  password: "${HIVE_PASSWORD}"
  save_mode: "append"
  table_prefix: "data_"
  partition_columns:
    - "year"
    - "month"
    - "day"

# NVD (National Vulnerability Database) sources (additional source)
nvd_sources:
  - name: "cve_data"
    enabled: true
    api_key: "${NVD_API_KEY}" # Optional API key for higher rate limits
    delay_between_requests: 6 # Seconds between requests to avoid rate limiting
    results_per_page: 2000 # Number of results per page
    max_pages: 10 # Maximum number of pages to fetch (null for all)
    start_date: "2023-01-01T00:00:00.000" # Optional start date
    end_date: null # Optional end date (null for current date)
    incremental_load: true # Whether to use incremental loading
    state_path: "./nvd_state" # Directory to store state information for incremental loads
    additional_query_params: # Additional query parameters for the NVD API
      cpeMatchString: null # Optional CPE match string
      cvssV2Metrics: null # Optional CVSS v2 metrics
      cvssV3Metrics: null # Optional CVSS v3 metrics
      keywordSearch: null # Optional keyword search
    # Classification thresholds can be overridden per source
    classification_thresholds:
      bronze:
        completeness: 0.6
        accuracy: 0.6
        timeliness: 0.7

# Elasticsearch configuration (optional secondary data store)
elasticsearch:
  hosts:
    - "http://${ES_HOST:localhost}:${ES_PORT:9200}"
  username: "${ES_USERNAME}"
  password: "${ES_PASSWORD}"
  verify_certs: true
  kibana_url: "http://${KIBANA_HOST:localhost}:${KIBANA_PORT:5601}"
  indices:
    metrics: "data-pipeline-metrics"
    bronze: "bronze-data"
    silver: "silver-data"
    gold: "gold-data"
    rejected: "rejected-data"
  options:
    refresh_on_write: true
    number_of_shards: 1
    number_of_replicas: 0